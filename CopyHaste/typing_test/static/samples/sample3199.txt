def cost_matrix(contingency_table):
	return make_cost_matrix(contingency_table, lambda cost: sys.maxsize - cost)
def match_labels(contingency_table):
	m = Munkres()
	ind = m.compute(cost_matrix(contingency_table.as_matrix()))
	return rearrange_columns(ind, contingency_table.copy())
def true_positives(contingency_table):
	return np.diag(contingency_table)
def false_negatives(contingency_table):
	fn = [0] * contingency_table.shape[0]
	for i in range(contingency_table.shape[0]):
		for j in range(contingency_table.shape[1]):
			if i != j:
				fn[i] += contingency_table.iloc[i, j]
	return fn
def false_positives(contingency_table):
	fp = [0] * contingency_table.shape[1]
	for j in range(contingency_table.shape[1]):
		for i in range(contingency_table.shape[0]):
			if i != j:
				fp[j] += contingency_table.iloc[i, j]
	return fp
def recall_macro(contingency_table):
	tp = true_positives(contingency_table)
	fn = false_negatives(contingency_table)
	return float(np.average(tp) / (np.average(tp) + np.average(fn)))
def precision_macro(contingency_table):
	tp = true_positives(contingency_table)
	fp = false_positives(contingency_table)
	return float(np.average(tp) / (np.average(tp) + np.average(fp)))
def f1_score_macro(contingency_table):
	tp = true_positives(contingency_table)
	fn = false_negatives(contingency_table)
	fp = false_positives(contingency_table)
	return float(2.0 * np.average(tp) / (2.0 * np.average(tp) + np.average(fn) + np.average(fp)))
def rearrange_columns(indices, contingency_table):
	cn_in = contingency_table.columns.tolist()
	cn_out = [None] * len(cn_in)
	moved = [False] * len(cn_out)
	for i, j in indices:
		cn_out[i] = cn_in[j]
		moved[j] = True
	last = sum(moved)
	for i, mv in enumerate(moved):
		if not mv:
			cn_out[last] = cn_in[i]
			last += 1
	return contingency_table[cn_out]
def over_clustered(dataFrame):
	return dataFrame.shape[0] > dataFrame.shape[1]
def add_padding_columns(dataFrame):
	shp = dataFrame.shape
	nDummy = shp[0] - shp[1]
	i = 0
	while i < nDummy:
		dataFrame['dummy' + str(i)] = [0] * shp[0]
		i += 1
def print_table(df):
	a = np.empty((df.shape[0], df.shape[1]+1), dtype=int)
	a[:, 1:] = df
	a[:, 0] = np.sum(df, axis=1)
	b = np.empty((a.shape[0]+1, a.shape[1]), dtype=int)
	b[1:, :] = a
	b[0, :] = np.sum(a, axis=0)
	print pd.DataFrame(b,
					   columns=['Sum'] + df.columns.values.tolist(),
					   index=['Sum'] + df.index.values.tolist())
if __name__ == '__main__':
	parser = argparse.ArgumentParser(description='Calculate F1 metric')
	parser.add_argument('-s', '--min-score', type=int, help='minimum truth object score', default=0)
	parser.add_argument('truth', nargs=1, help='truth table in yaml format')
	parser.add_argument('pred', nargs=1, help='prediction in MCL format')
	parser.add_argument('output', nargs='?', type=argparse.FileType('w'), default=sys.stdout, help='Output file')
	args = parser.parse_args()
	print 'Reading truth table...'
	truth = tt.read_truth(args.truth[0], args.min_score)
	print 'Reading prediction...'
	pred = tt.read_mcl(args.pred[0])
	print 'Creating contingency table...'
	ct = tt.crosstab(truth.hard(), pred.hard())
	print
	print 'Contigency table [rows=truth, cols=prediction] contains {0} elements'.format(ct.shape[0] * ct.shape[1])
	print_table(ct)
	print
	if over_clustered(ct):
		add_padding_columns(ct)
		print 'Squaring table with dummy classes'
		print_table(ct)
		print
	print 'Matching labels using Munkres, algorithm suffers
	mct = match_labels(ct)
	print 'Aligned contigency table'
	print_table(mct)
	score = {'f1': f1_score_macro(mct),
			 'recall': recall_macro(mct),
			 'prec': precision_macro(mct)}
	pipeline_utils.write_to_stream(args.output, score)