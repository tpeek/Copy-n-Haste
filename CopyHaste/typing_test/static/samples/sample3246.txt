try:
except ImportError:
__author__ = 'Brian Quinlan (brian@sweetapp.com)'
_thread_references = set()
_shutdown = False
def _python_exit():
	global _shutdown
	_shutdown = True
	for thread_reference in _thread_references:
		thread = thread_reference()
		if thread is not None:
			thread.join()
def _remove_dead_thread_references():
	for thread_reference in set(_thread_references):
		if thread_reference() is None:
			_thread_references.discard(thread_reference)
EXTRA_QUEUED_CALLS = 1
class _WorkItem(object):
	def __init__(self, future, fn, args, kwargs):
		self.future = future
		self.fn = fn
		self.args = args
		self.kwargs = kwargs
class _ResultItem(object):
	def __init__(self, work_id, exception=None, result=None):
		self.work_id = work_id
		self.exception = exception
		self.result = result
class _CallItem(object):
	def __init__(self, work_id, fn, args, kwargs):
		self.work_id = work_id
		self.fn = fn
		self.args = args
		self.kwargs = kwargs
def _process_worker(call_queue, result_queue, shutdown):
	while True:
		try:
			call_item = call_queue.get(block=True, timeout=0.1)
		except queue.Empty:
			if shutdown.is_set():
				return
		else:
			try:
				r = call_item.fn(*call_item.args, **call_item.kwargs)
			except BaseException:
				e = sys.exc_info()[1]
				result_queue.put(_ResultItem(call_item.work_id,
											 exception=e))
			else:
				result_queue.put(_ResultItem(call_item.work_id,
											 result=r))
def _add_call_item_to_queue(pending_work_items,
							work_ids,
							call_queue):
	while True:
		if call_queue.full():
			return
		try:
			work_id = work_ids.get(block=False)
		except queue.Empty:
			return
		else:
			work_item = pending_work_items[work_id]
			if work_item.future.set_running_or_notify_cancel():
				call_queue.put(_CallItem(work_id,
										 work_item.fn,
										 work_item.args,
										 work_item.kwargs),
							   block=True)
			else:
				del pending_work_items[work_id]
				continue
def _queue_manangement_worker(executor_reference,
							  processes,
							  pending_work_items,
							  work_ids_queue,
							  call_queue,
							  result_queue,
							  shutdown_process_event):
	while True:
		_add_call_item_to_queue(pending_work_items,
								work_ids_queue,
								call_queue)
		try:
			result_item = result_queue.get(block=True, timeout=0.1)
		except queue.Empty:
			executor = executor_reference()
			if _shutdown or executor is None or executor._shutdown_thread:
				if not pending_work_items:
					shutdown_process_event.set()
					for p in processes:
						p.join()
					return
			del executor
		else:
			work_item = pending_work_items[result_item.work_id]
			del pending_work_items[result_item.work_id]
			if result_item.exception:
				work_item.future.set_exception(result_item.exception)
			else:
				work_item.future.set_result(result_item.result)
class ProcessPoolExecutor(_base.Executor):
	def __init__(self, max_workers=None):
		_remove_dead_thread_references()
		if max_workers is None:
			self._max_workers = multiprocessing.cpu_count()
		else:
			self._max_workers = max_workers
		self._call_queue = multiprocessing.Queue(self._max_workers +
												 EXTRA_QUEUED_CALLS)
		self._result_queue = multiprocessing.Queue()
		self._work_ids = queue.Queue()
		self._queue_management_thread = None
		self._processes = set()
		self._shutdown_thread = False
		self._shutdown_process_event = multiprocessing.Event()
		self._shutdown_lock = threading.Lock()
		self._queue_count = 0
		self._pending_work_items = {}
	def _start_queue_management_thread(self):
		if self._queue_management_thread is None:
			self._queue_management_thread = threading.Thread(
					target=_queue_manangement_worker,
					args=(weakref.ref(self),
						  self._processes,
						  self._pending_work_items,
						  self._work_ids,
						  self._call_queue,
						  self._result_queue,
						  self._shutdown_process_event))
			self._queue_management_thread.daemon = True
			self._queue_management_thread.start()
			_thread_references.add(weakref.ref(self._queue_management_thread))
	def _adjust_process_count(self):
		for _ in range(len(self._processes), self._max_workers):
			p = multiprocessing.Process(
					target=_process_worker,
					args=(self._call_queue,
						  self._result_queue,
						  self._shutdown_process_event))
			p.start()
			self._processes.add(p)
	def submit(self, fn, *args, **kwargs):
		with self._shutdown_lock:
			if self._shutdown_thread:
				raise RuntimeError('cannot schedule new futures after shutdown')
			f = _base.Future()
			w = _WorkItem(f, fn, args, kwargs)
			self._pending_work_items[self._queue_count] = w
			self._work_ids.put(self._queue_count)
			self._queue_count += 1
			self._start_queue_management_thread()
			self._adjust_process_count()
			return f
	submit.__doc__ = _base.Executor.submit.__doc__
	def shutdown(self, wait=True):
		with self._shutdown_lock:
			self._shutdown_thread = True
		if wait:
			if self._queue_management_thread:
				self._queue_management_thread.join()
		self._queue_management_thread = None
		self._call_queue = None
		self._result_queue = None
		self._shutdown_process_event = None
		self._processes = None
	shutdown.__doc__ = _base.Executor.shutdown.__doc__
atexit.register(_python_exit)