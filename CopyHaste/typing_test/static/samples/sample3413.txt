if __name__ == "__main__":
	if len(sys.argv) > 1:
		print("Usage: simple_params_example", file=sys.stderr)
		exit(1)
	sc = SparkContext(appName="PythonSimpleParamsExample")
	sqlContext = SQLContext(sc)
	training = sc.parallelize([
		LabeledPoint(1.0, DenseVector([0.0, 1.1, 0.1])),
		LabeledPoint(0.0, DenseVector([2.0, 1.0, -1.0])),
		LabeledPoint(0.0, DenseVector([2.0, 1.3, 1.0])),
		LabeledPoint(1.0, DenseVector([0.0, 1.2, -0.5]))]).toDF()
	lr = LogisticRegression(maxIter=10)
	print("LogisticRegression parameters:\n" + lr.explainParams() + "\n")
	lr.setRegParam(0.01)
	model1 = lr.fit(training)
	print("Model 1 was fit using parameters:\n")
	pprint.pprint(model1.extractParamMap())
	paramMap = {lr.maxIter: 20, lr.thresholds: [0.45, 0.55], lr.probabilityCol: "myProbability"}
	model2 = lr.fit(training, paramMap)
	print("Model 2 was fit using parameters:\n")
	pprint.pprint(model2.extractParamMap())
	test = sc.parallelize([
		LabeledPoint(1.0, DenseVector([-1.0, 1.5, 1.3])),
		LabeledPoint(0.0, DenseVector([3.0, 2.0, -0.1])),
		LabeledPoint(0.0, DenseVector([0.0, 2.2, -1.5]))]).toDF()
	result = model2.transform(test) \
		.select("features", "label", "myProbability", "prediction") \
		.collect()
	for row in result:
		print("features=%s,label=%s -> prob=%s, prediction=%s"
			  % (row.features, row.label, row.myProbability, row.prediction))
	sc.stop()