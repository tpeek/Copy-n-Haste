if __name__ == "__main__":
	if len(sys.argv) != 3:
		print(, file=sys.stderr)
		exit(-1)
	host = sys.argv[1]
	table = sys.argv[2]
	sc = SparkContext(appName="HBaseInputFormat")
	conf = {"hbase.zookeeper.quorum": host, "hbase.mapreduce.inputtable": table}
	if len(sys.argv) > 3:
		conf = {"hbase.zookeeper.quorum": host, "zookeeper.znode.parent": sys.argv[3],
				"hbase.mapreduce.inputtable": table}
	keyConv = "org.apache.spark.examples.pythonconverters.ImmutableBytesWritableToStringConverter"
	valueConv = "org.apache.spark.examples.pythonconverters.HBaseResultToStringConverter"
	hbase_rdd = sc.newAPIHadoopRDD(
		"org.apache.hadoop.hbase.mapreduce.TableInputFormat",
		"org.apache.hadoop.hbase.io.ImmutableBytesWritable",
		"org.apache.hadoop.hbase.client.Result",
		keyConverter=keyConv,
		valueConverter=valueConv,
		conf=conf)
	hbase_rdd = hbase_rdd.flatMapValues(lambda v: v.split("\n")).mapValues(json.loads)
	output = hbase_rdd.collect()
	for (k, v) in output:
		print((k, v))
	sc.stop()