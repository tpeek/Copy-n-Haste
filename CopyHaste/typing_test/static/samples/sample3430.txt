def summarize(dataset):
	print("schema: %s" % dataset.schema().json())
	labels = dataset.map(lambda r: r.label)
	print("label average: %f" % labels.mean())
	features = dataset.map(lambda r: r.features)
	summary = Statistics.colStats(features)
	print("features average: %r" % summary.mean())
if __name__ == "__main__":
	if len(sys.argv) > 2:
		print("Usage: dataset_example.py <libsvm file>", file=sys.stderr)
		exit(-1)
	sc = SparkContext(appName="DatasetExample")
	sqlContext = SQLContext(sc)
	if len(sys.argv) == 2:
		input = sys.argv[1]
	else:
		input = "data/mllib/sample_libsvm_data.txt"
	points = MLUtils.loadLibSVMFile(sc, input)
	dataset0 = sqlContext.inferSchema(points).setName("dataset0").cache()
	summarize(dataset0)
	tempdir = tempfile.NamedTemporaryFile(delete=False).name
	os.unlink(tempdir)
	print("Save dataset as a Parquet file to %s." % tempdir)
	dataset0.saveAsParquetFile(tempdir)
	print("Load it back and summarize it again.")
	dataset1 = sqlContext.parquetFile(tempdir).setName("dataset1").cache()
	summarize(dataset1)
	shutil.rmtree(tempdir)