stoplist = stopwords.words('english')
stoplist.extend(stopwords.words('french'))
stoplist.extend(["also","said","work","one","two","three"])
with codecs.open(sys.argv[1], 'r', encoding='utf-8') as f:
  data = json.loads(f.read())
  documents = [x['description'] for x in data]
  dictionary = corpora.Dictionary(text.encode('ascii', 'ignore').lower().split() for text in documents)
  dictionary.filter_extremes(no_below=1, no_above=0.8)
  stop_ids = [dictionary.token2id[stopword] for stopword in stoplist if stopword in dictionary.token2id]
  once_ids = [tokenid for tokenid, docfreq in dictionary.dfs.iteritems() if docfreq == 3]
  dictionary.filter_tokens(stop_ids + once_ids)
  dictionary.compactify()
  corpus = [dictionary.doc2bow(line.encode('a